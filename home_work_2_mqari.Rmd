---
title: "home_work_2_mq"
author: "Mubashira Qari"
date: "2026-02-08"
output: html_document
---

# Prepare data for decomposition 
# we make transformation and adjustment
# We devide the data by population to make it totals per capita


```{r}
library(fpp3)
global_economy

```


```{r}
global_economy |>
  filter(Country == "Australia") |>
autoplot(GDP)
```
# Per Capita Adjustment
To understand the growth of the economy it should be relative to the size of the population
for that we divide by population and then model that per capita data
This is what it means adjustment


```{r}
global_economy |>
  filter(Country == "Australia") |>
  autoplot(GDP/Population) +
  labs(title= "GDP per capita", y = "$US")
```
# Inflation Adjustment + Calendar Adjustment

```{r}
aus_retail
```

```{r}
print_retail <- aus_retail |>
  filter(Industry == "Newspaper and book retailing") |>
  group_by(Industry) |>
  index_by(Year = year(Month)) |>
  summarise(Turnover = sum(Turnover))

print_retail
```

```{r}
print_retail |> autoplot(Turnover)
```

```{r}

aus_economy <- global_economy |>
  filter(Code == "AUS")

aus_economy
```
```{r}

print_retail |>
  left_join(aus_economy, by = "Year")

```


```{r}
print_retail |>
  left_join(aus_economy, by = "Year") |>
  mutate(Adjusted_turnover = Turnover / CPI * 100) |>
  pivot_longer(c(Turnover, Adjusted_turnover),
               values_to = "Turnover") |>
  mutate(name = factor(name,
         levels=c("Turnover","Adjusted_turnover"))) |>
  ggplot(aes(x = Year, y = Turnover)) +
  geom_line() +
  facet_grid(name ~ ., scales = "free_y") +
  labs(title = "Turnover: Australian print media industry",
       y = "$AU")
```

# Mathematical Adjustment
If the data show different variation at different levels of the series, then a transformation can be useful

```{r}

```
```{r}
food <-aus_retail |>
  filter(Industry ==  "Food retailing") |>
  summarise(Turnover = sum(Turnover)) 

food
```
Idea of transformation is to make graph more homogeneous (more even variation across the scale of data).

```{r}
# 2. Plot the data
food |>
  autoplot(Turnover) +
  labs(
    title = "Total Australian Food Retailing Turnover",
    y = "Turnover (million $AUD)",
    x = "Month"
  )
```

```{r}
# litter more even variation 
food |> autoplot(sqrt(Turnover))+
  labs(y = "Square root turnover")

```
```{r}
# litter more even variation with cube root
food |> autoplot(Turnover^(1/3))+
  labs(y = "Cube root turnover")
```
```{r}
#  even variation with log root, remove heterogeneity so you fit more simple model
food |> autoplot(log(Turnover)) +
  labs(y = "Log root turnover")
```
```{r}
# this is taking too far with more variation in the beginning and less on the end. So log is best solution
food |> autoplot(-1/Turnover) +
  labs(y = "Inverse turnover")
```
# Box Cox Transformation
Each of these transformations can be thought of in a continuous scale of transformations which are known as Box Cox transformations. 
Lamda describe how strong transformation is
If Lambda is 0, we take natural logrithm
If Lambda is not equal to 0, we do this tranformation which raises the data to the power of Lambda and then subracts 1/lambda 
Reason for sign is to be able to deal with negative value - called pickle docson tranformation

When we start making lambda smaller, transformation gets stronger and we choose where transformation is even
It remove heterogeneity in the data

```{r}
lambda <- aus_production |>
  features(Gas, features = guerrero) |>
  pull(lambda_guerrero)
aus_production |>
  autoplot(box_cox(Gas, lambda)) +
  labs(y = "",
       title = latex2exp::TeX(paste0(
         "Transformed gas production with $\\lambda$ = ",
         round(lambda,2))))
```

There is function that automatically choose lambda called guerrero transformation and gives you most even distribution across the scale

```{r}
food |>
  features(Turnover, features =guerrero)
```
Transformation have large impact on predictions

another log 1P tranformation when data have zeros but you want something like log so it adds 1 and then takes a log.

Choosing log has nice effect of forcing all forecast to stay positive

When you work on the tranform data, then you have to back tranform to see the results on the original scale.

Using Fable package, we don't have to undo transformations when doing forcasting later on since will be done automatically


#Now Prepare data for decomposition 
Split the data into these components
Look for Trend, Cyclic, Seasonal

Trend: pattern exist when there is long tem increase or decrease in the data
Cycle: pattern exist when data exhibit rise and fall that are not for fixed period
Seasonal: pattern exist when a series is influenced by seasonal factor (eg,quarter, month,day of week)

In time series decomposition we split the data into following

y = f(S, T, R)

y =data at period t
T= trend cycle component at t (combine trend and cycle)
S= Sesonal cycle component at t
R = remainder cycle component at t

and it joined togather in one of two ways:

Additive Decomposition: y =S+T+R
Multiplicative Decomposition: y = SX T X R

```{r}
us_retail_employment
```

```{r}
us_retail_employment <- us_employment |>
  filter(year(Month) >= 1990, Title == "Retail Trade") |>
  select(-Series_ID)
autoplot(us_retail_employment, Employed) +
  labs(y = "Persons (thousands)",
       title = "Total employment in US retail")
```

```{r}
# Seasonal Trend and lowest decomposition
dcmp <- us_retail_employment |>
  model(stl = STL(Employed))
components(dcmp)

```

Plot the data
```{r}
components(dcmp) |> autoplot()
```

```{r}
components(dcmp) |>
  as_tsibble() |>
  autoplot(Employed, colour="gray") +
  geom_line(aes(y=trend), colour = "#D55E00") +
  labs(
    y = "Persons (thousands)",
    title = "Total employment in US retail"
  )
```
Sub Series Plot of seasonal component: show the shape of seasonality

```{r}
components(dcmp) |> gg_subseries(season_year)
```

Seasonally Adjusted Data: Remove the seasonality
Helps see fluctuations in the data that are not affected by the seasonal pattern
most commonly used in the decomposition

Additive Decomposition: y-S = T+R

Mutiplicative Decomposition: y/S = T X R

```{r}
components(dcmp) |>
  as_tsibble() |>
  autoplot(Employed, colour = "gray") +
  geom_line(aes(y=season_adjust), colour = "#0072B2") +
  labs(y = "Persons (thousands)",
       title = "Total employment in US retail")
```

If the seasonal component is removed from the original data, the resulting values are the “seasonally adjusted” data.

For an additive decomposition, the seasonally adjusted data are given by  
y-S
 , and for multiplicative data, the seasonally adjusted values are obtained using  
y/S

Its more wiggly because you remove the seasonality and what left is trend and remainder

it is better to use trend-cycle component to look for turning point

# Classical Decomposition

for performing classical decomposition, first thing to look is the trend component and its estimate
and for estimate of trend component we look into moving averages

```{r}
global_economy |>
  filter(Country == "Australia") |>
  autoplot(Exports) +
  labs(y = "% of GDP", title = "Total Australian exports")
```

disadvantage: Can not be estimated near the end points. we loose point at the end

to overcome this

This is easily computed using slide_dbl() from the slider package which applies a function to “sliding” time windows. In this case, we use the mean() function with a window of size 5.

```{r}
aus_exports <- global_economy |>
  filter(Country == "Australia") |>
  mutate(
    `5-MA` = slider::slide_dbl(Exports, mean,
                .before = 2, .after = 2, .complete = TRUE)
  )
```

To see what the trend-cycle estimate looks like, we plot it along with the original data in Figure

```{r}
aus_exports |>
  autoplot(Exports) +
  geom_line(aes(y = `5-MA`), colour = "#D55E00") +
  labs(y = "% of GDP",
       title = "Total Australian exports")
```
Moving averages of moving averages

```{r}
beer <- aus_production |>
  filter(year(Quarter) >= 1992) |>
  select(Quarter, Beer)
beer_ma <- beer |>
  mutate(
    `4-MA` = slider::slide_dbl(Beer, mean,
                .before = 1, .after = 2, .complete = TRUE),
    `2x4-MA` = slider::slide_dbl(`4-MA`, mean,
                .before = 1, .after = 0, .complete = TRUE)
  )
```

Example: Employment in the US retail sector

```{r}
us_retail_employment_ma <- us_retail_employment |>
  mutate(
    `12-MA` = slider::slide_dbl(Employed, mean,
                .before = 5, .after = 6, .complete = TRUE),
    `2x12-MA` = slider::slide_dbl(`12-MA`, mean,
                .before = 1, .after = 0, .complete = TRUE)
  )
us_retail_employment_ma |>
  autoplot(Employed, colour = "gray") +
  geom_line(aes(y = `2x12-MA`), colour = "#D55E00") +
  labs(y = "Persons (thousands)",
       title = "Total employment in US retail")
```
Weighted moving averages
Combinations of moving averages result in weighted moving averages

# Classical Decomposition

 Two types of Classical decomposition
 
 Additive: where components add
 Mutiplicative: where components mutiply
 
 First step is to use moving averages to get trend so I can estimate
 
 Estimate T Trend using (2xm)  Moving Average if m is even
 otherwise T using m Moving Average
 
 Next Calculate de-trended series
 
 Additive decomposition: y-T
 Mutiplicative decomposition: y/T
 
 De-trending remove smoothed series T from and leave S and R
 
```{r}
us_retail_employment |>
  model(
    classical_decomposition(Employed, type = "additive")
  ) |>
  components() |>
  autoplot() +
  labs(title = "Classical additive decomposition of total
                  US retail employment")
```
 
X-11 Decomposition Method:

```{r}
x11_dcmp |>
  ggplot(aes(x = Month)) +
  geom_line(aes(y = Employed, colour = "Data")) +
  geom_line(aes(y = season_adjust,
                colour = "Seasonally Adjusted")) +
  geom_line(aes(y = trend, colour = "Trend")) +
  labs(y = "Persons (thousands)",
       title = "Total employment in US retail") +
  scale_colour_manual(
    values = c("gray", "#0072B2", "#D55E00"),
    breaks = c("Data", "Seasonally Adjusted", "Trend")
  )
```

